import pandas as pd
import os
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report

# ç›´æ¥æŒ‡å®šç»“æœCSVè·¯å¾„
result_csv_path = "results/gpt-4o-mini-2024-07-18_2025-07-14_14-07-57_answers.csv"

def analyze_results(result_csv_path):
    # æ£€æŸ¥è·¯å¾„æœ‰æ•ˆæ€§
    if not os.path.exists(result_csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {result_csv_path}")
        return

    # è¯»å–CSV
    df = pd.read_csv(result_csv_path)

    # å»é™¤ç©ºç­”æ¡ˆæˆ–æ— æ•ˆç­”æ¡ˆ
    df = df[df["model_answer"].isin(["A", "B", "C", "D"])]

    y_true = df["correct_answer"]
    y_pred = df["model_answer"]

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average="macro", zero_division=0)
    recall = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1 = f1_score(y_true, y_pred, average="macro", zero_division=0)

    # è¾“å‡ºæ§åˆ¶
    print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision (macro): {precision:.4f}")
    print(f"Recall (macro): {recall:.4f}")
    print(f"F1-score (macro): {f1:.4f}")
    print("\nğŸ“„ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_true, y_pred, zero_division=0))

    # ä¿å­˜ä¸ºæ–° CSV
    data_output_path = result_csv_path.replace(".csv", "_analysis.csv")
    summary_df = pd.DataFrame({
        "Metric": ["Accuracy", "Precision (macro)", "Recall (macro)", "F1-score (macro)"],
        "Score": [accuracy, precision, recall, f1]
    })

    summary_df.to_csv(data_output_path, index=False)
    print(f"\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {data_output_path}")

# ç›´æ¥æ‰§è¡Œ
if __name__ == "__main__":
    analyze_results(result_csv_path)
